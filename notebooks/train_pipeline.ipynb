{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d136ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from src.dataset import BrainMRIDataset\n",
    "\n",
    "# Seting data path\n",
    "DATA_PATH = '../data/mri-segmentation/kaggle_3m/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f70c865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3929 image-mask pairs\n",
      "Found 2720 image-mask pairs\n",
      "Found 475 image-mask pairs\n",
      "Found 734 image-mask pairs\n",
      "Train dataset: 2720 slices\n",
      "Val dataset: 475 slices\n",
      "Test dataset: 734 slices\n"
     ]
    }
   ],
   "source": [
    "# Get all patient IDs\n",
    "dataset = BrainMRIDataset(data_path=DATA_PATH)\n",
    "patient_ids = list(set([os.path.basename(os.path.dirname(path)) for path in dataset.image_paths]))\n",
    "\n",
    "# Split patients\n",
    "train_patients, test_patients = train_test_split(patient_ids, test_size=0.15, random_state=42)\n",
    "train_patients, val_patients = train_test_split(train_patients, test_size=0.176, random_state=42)\n",
    "\n",
    "# Create filtered datasets\n",
    "train_dataset = BrainMRIDataset(DATA_PATH, patient_list=train_patients)\n",
    "val_dataset = BrainMRIDataset(DATA_PATH, patient_list=val_patients)\n",
    "test_dataset = BrainMRIDataset(DATA_PATH, patient_list=test_patients)\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)} slices\")\n",
    "print(f\"Val dataset: {len(val_dataset)} slices\")\n",
    "print(f\"Test dataset: {len(test_dataset)} slices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "043ec4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transforms defined!\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Training augmentations\n",
    "train_transform = A.Compose([\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.VerticalFlip(p=0.5),\n",
    "    A.RandomRotate90(p=0.5),\n",
    "    A.Affine(scale=(0.9, 1.1), translate_percent=(-0.1, 0.1), rotate=(-15, 15), p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "# Validation/Test augmentations\n",
    "val_transform = A.Compose([\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "print(\"Transforms defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d96beecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2720 image-mask pairs\n",
      "Found 475 image-mask pairs\n",
      "Found 734 image-mask pairs\n",
      "Datasets created with transforms!\n",
      "Train: 2720 | Val: 475 | Test: 734\n"
     ]
    }
   ],
   "source": [
    "# Recreate datasets with transforms\n",
    "train_dataset = BrainMRIDataset(DATA_PATH, transform=train_transform, patient_list=train_patients)\n",
    "val_dataset = BrainMRIDataset(DATA_PATH, transform=val_transform, patient_list=val_patients)\n",
    "test_dataset = BrainMRIDataset(DATA_PATH, transform=val_transform, patient_list=test_patients)\n",
    "\n",
    "print(\"Datasets created with transforms!\")\n",
    "print(f\"Train: {len(train_dataset)} | Val: {len(val_dataset)} | Test: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0e468d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image type: <class 'torch.Tensor'>\n",
      "Image shape: torch.Size([3, 256, 256])\n",
      "Mask shape: torch.Size([256, 256])\n",
      "Image dtype: torch.float32\n",
      "Image range: [-2.12, 1.53]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Get one sample\n",
    "image, mask = train_dataset[0]\n",
    "\n",
    "print(f\"Image type: {type(image)}\")\n",
    "print(f\"Image shape: {image.shape}\")  # Should be (3, 256, 256) - PyTorch format\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "print(f\"Image dtype: {image.dtype}\")\n",
    "print(f\"Image range: [{image.min():.2f}, {image.max():.2f}]\")  # Should be normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b750c974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train batches: 170\n",
      "Val batches: 30\n",
      "Test batches: 46\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}\")\n",
    "print(f\"Val batches: {len(val_loader)}\")\n",
    "print(f\"Test batches: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2207cdb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([16, 3, 256, 256])\n",
      "Batch masks shape: torch.Size([16, 256, 256])\n",
      "Images dtype: torch.float32\n",
      "Masks dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "images, masks = next(iter(train_loader))\n",
    "\n",
    "print(f\"Batch images shape: {images.shape}\")  # Should be (16, 3, 256, 256)\n",
    "print(f\"Batch masks shape: {masks.shape}\")    # Should be (16, 256, 256)\n",
    "print(f\"Images dtype: {images.dtype}\")\n",
    "print(f\"Masks dtype: {masks.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6dcac2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "model = smp.Unet(\n",
    "    encoder_name=\"resnet50\",        # pretrained ResNet50 as encoder\n",
    "    encoder_weights=\"imagenet\",     # using ImageNet pretrained weights\n",
    "    in_channels=3,                  # RGB images\n",
    "    classes=1,                      # binary segmentation (tumor vs background)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace7c882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Unet                                          [1, 1, 256, 256]          --\n",
       "├─ResNetEncoder: 1-1                          [1, 3, 256, 256]          --\n",
       "│    └─Conv2d: 2-1                            [1, 64, 128, 128]         9,408\n",
       "│    └─BatchNorm2d: 2-2                       [1, 64, 128, 128]         128\n",
       "│    └─ReLU: 2-3                              [1, 64, 128, 128]         --\n",
       "│    └─MaxPool2d: 2-4                         [1, 64, 64, 64]           --\n",
       "│    └─Sequential: 2-5                        [1, 256, 64, 64]          --\n",
       "│    │    └─Bottleneck: 3-1                   [1, 256, 64, 64]          75,008\n",
       "│    │    └─Bottleneck: 3-2                   [1, 256, 64, 64]          70,400\n",
       "│    │    └─Bottleneck: 3-3                   [1, 256, 64, 64]          70,400\n",
       "│    └─Sequential: 2-6                        [1, 512, 32, 32]          --\n",
       "│    │    └─Bottleneck: 3-4                   [1, 512, 32, 32]          379,392\n",
       "│    │    └─Bottleneck: 3-5                   [1, 512, 32, 32]          280,064\n",
       "│    │    └─Bottleneck: 3-6                   [1, 512, 32, 32]          280,064\n",
       "│    │    └─Bottleneck: 3-7                   [1, 512, 32, 32]          280,064\n",
       "│    └─Sequential: 2-7                        [1, 1024, 16, 16]         --\n",
       "│    │    └─Bottleneck: 3-8                   [1, 1024, 16, 16]         1,512,448\n",
       "│    │    └─Bottleneck: 3-9                   [1, 1024, 16, 16]         1,117,184\n",
       "│    │    └─Bottleneck: 3-10                  [1, 1024, 16, 16]         1,117,184\n",
       "│    │    └─Bottleneck: 3-11                  [1, 1024, 16, 16]         1,117,184\n",
       "│    │    └─Bottleneck: 3-12                  [1, 1024, 16, 16]         1,117,184\n",
       "│    │    └─Bottleneck: 3-13                  [1, 1024, 16, 16]         1,117,184\n",
       "│    └─Sequential: 2-8                        [1, 2048, 8, 8]           --\n",
       "│    │    └─Bottleneck: 3-14                  [1, 2048, 8, 8]           6,039,552\n",
       "│    │    └─Bottleneck: 3-15                  [1, 2048, 8, 8]           4,462,592\n",
       "│    │    └─Bottleneck: 3-16                  [1, 2048, 8, 8]           4,462,592\n",
       "├─UnetDecoder: 1-2                            [1, 16, 256, 256]         --\n",
       "│    └─Identity: 2-9                          [1, 2048, 8, 8]           --\n",
       "│    └─ModuleList: 2-10                       --                        --\n",
       "│    │    └─UnetDecoderBlock: 3-17            [1, 256, 16, 16]          7,668,736\n",
       "│    │    └─UnetDecoderBlock: 3-18            [1, 128, 32, 32]          1,032,704\n",
       "│    │    └─UnetDecoderBlock: 3-19            [1, 64, 64, 64]           258,304\n",
       "│    │    └─UnetDecoderBlock: 3-20            [1, 32, 128, 128]         46,208\n",
       "│    │    └─UnetDecoderBlock: 3-21            [1, 16, 256, 256]         6,976\n",
       "├─SegmentationHead: 1-3                       [1, 1, 256, 256]          --\n",
       "│    └─Conv2d: 2-11                           [1, 1, 256, 256]          145\n",
       "│    └─Identity: 2-12                         [1, 1, 256, 256]          --\n",
       "│    └─Activation: 2-13                       [1, 1, 256, 256]          --\n",
       "│    │    └─Identity: 3-22                    [1, 1, 256, 256]          --\n",
       "===============================================================================================\n",
       "Total params: 32,521,105\n",
       "Trainable params: 32,521,105\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 10.63\n",
       "===============================================================================================\n",
       "Input size (MB): 0.79\n",
       "Forward/backward pass size (MB): 297.80\n",
       "Params size (MB): 130.08\n",
       "Estimated Total Size (MB): 428.67\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 3, 256, 256))  # batch_size=1, 3 channels, 256x256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa622b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Loss: Dice Loss\n",
      "Optimizer: Adam (lr=1e-4)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Loss function - Binary Cross Entropy with Logits + Dice Loss\n",
    "criterion = smp.losses.DiceLoss(mode='binary')\n",
    "\n",
    "# Optimizer - Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Device\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Loss: Dice Loss\")\n",
    "print(f\"Optimizer: Adam (lr=1e-4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78b04193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_dice_score(outputs, masks, threshold=0.5):\n",
    "    \"\"\"Calculate Dice score for batch\"\"\"\n",
    "    # Apply sigmoid to get probabilities\n",
    "    preds = torch.sigmoid(outputs) > threshold\n",
    "    preds = preds.float()\n",
    "    \n",
    "    # Flatten\n",
    "    preds = preds.view(-1)\n",
    "    masks = masks.view(-1)\n",
    "    \n",
    "    # Dice = 2 * |X ∩ Y| / (|X| + |Y|)\n",
    "    intersection = (preds * masks).sum()\n",
    "    dice = (2. * intersection) / (preds.sum() + masks.sum() + 1e-8)\n",
    "    \n",
    "    return dice.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d98863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc='Training', leave=False)\n",
    "    \n",
    "    for images, masks in pbar:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device).unsqueeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, masks)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate dice score\n",
    "        dice = calculate_dice_score(outputs, masks)\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        running_dice += dice\n",
    "        \n",
    "        pbar.set_postfix({'loss': f'{loss.item():.4f}', 'dice': f'{dice:.4f}'})\n",
    "    \n",
    "    epoch_loss = running_loss / len(dataloader)\n",
    "    epoch_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return epoch_loss, epoch_dice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b2fed51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()  \n",
    "    running_loss = 0.0\n",
    "    running_dice = 0.0\n",
    "\n",
    "    pbar = tqdm(dataloader, desc='Evaluating', leave=False)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Calculate dice score\n",
    "            dice = calculate_dice_score(outputs, masks)\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_dice += dice\n",
    "            \n",
    "            pbar.set_postfix({'loss': f'{loss.item():.4f}', 'dice': f'{dice:.4f}'})\n",
    "    \n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    eval_dice = running_dice / len(dataloader)\n",
    "    \n",
    "    return eval_loss, eval_dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e906540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                      \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Train\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m train_loss, train_dice = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m train_losses.append(train_loss)\n\u001b[32m     16\u001b[39m train_dices.append(train_dice)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, dataloader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     12\u001b[39m masks = masks.to(device).unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m     14\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m loss = criterion(outputs, masks)\n\u001b[32m     17\u001b[39m loss.backward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/segmentation_models_pytorch/base/model.py:67\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_input_shape(x)\n\u001b[32m     66\u001b[39m features = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:168\u001b[39m, in \u001b[36mUnetDecoder.forward\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    166\u001b[39m     height, width = spatial_shapes[i + \u001b[32m1\u001b[39m]\n\u001b[32m    167\u001b[39m     skip_connection = skip_connections[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(skip_connections) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     x = \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1776\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1775\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1776\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/modules/module.py:1787\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1786\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1787\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1789\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1790\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/segmentation_models_pytorch/decoders/unet/decoder.py:50\u001b[39m, in \u001b[36mUnetDecoderBlock.forward\u001b[39m\u001b[34m(self, feature_map, target_height, target_width, skip_connection)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     45\u001b[39m     feature_map: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     skip_connection: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     49\u001b[39m ) -> torch.Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m     feature_map = \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m        \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_width\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m skip_connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m         feature_map = torch.cat([feature_map, skip_connection], dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/MyCodingDocs/GitHub projects/Rough Work/Brain Tumor Segmentation - UNet Model/myVenv/lib/python3.13/site-packages/torch/nn/functional.py:4812\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest1d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n\u001b[32m   4810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4811\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-argument-type]\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m4812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupsample_nearest2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28minput\u001b[39m.dim() == \u001b[32m5\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mnearest\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   4814\u001b[39m     \u001b[38;5;66;03m# pyrefly: ignore [bad-argument-type]\u001b[39;00m\n\u001b[32m   4815\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch._C._nn.upsample_nearest3d(\u001b[38;5;28minput\u001b[39m, output_size, scale_factors)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Training parameters\n",
    "NUM_EPOCHS = 20\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, val_losses = [], []\n",
    "train_dices, val_dices = [], []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_dice = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_dices.append(train_dice)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_dice = evaluate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_dices.append(val_dice)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Dice: {train_dice:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Dice: {val_dice:.4f}\")\n",
    "\n",
    "print(\"\\nTraining complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myVenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
